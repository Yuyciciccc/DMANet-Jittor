# import torch
# from torch.optim.lr_scheduler import _LRScheduler


# class WarmUpLR(_LRScheduler):
#     def __init__(self, optimizer, total_iters, last_epoch=-1):
#         self.total_iters = total_iters
#         super().__init__(optimizer, last_epoch)

#     def get_lr(self):
#         return [base_lr * self.last_epoch /(self.total_iters+1e-8) for base_lr in self.base_lrs]

import jittor as jt
from jittor.optim import LRScheduler

class WarmUpLR(LRScheduler):
    def __init__(self, optimizer, total_iters, last_epoch=-1):
        self.total_iters = total_iters
        super().__init__(optimizer, last_epoch)

    def get_lr(self):
        return [base_lr * self.last_epoch / (self.total_iters + 1e-8) for base_lr in self.base_lrs]

def main():
    # åˆ›å»ºä¸€ä¸ªç®€å•æ¨¡å‹
    model = nn.Linear(10, 2)

    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = SGD(model.parameters(), lr=0.1)

    # åˆ›å»º WarmUpLR å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆä¾‹å¦‚ï¼šå‰ 5 æ­¥è¿›è¡Œ warmupï¼‰
    warmup_scheduler = WarmUpLR(optimizer, total_iters=5)

    # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹
    print("ğŸ“ˆ Warm-up Learning Rate Progression:")
    for epoch in range(10):
        # æ¨¡æ‹Ÿ forward + backward
        x = jt.randn(8, 10)
        y = model(x)
        loss = y.sum()
        optimizer.step(loss)

        # è°ƒåº¦å™¨ step æ›´æ–°å­¦ä¹ ç‡
        warmup_scheduler.step()

        # æ‰“å°å½“å‰å­¦ä¹ ç‡
        current_lr = [group['lr'] for group in optimizer.param_groups]
        print(f"Epoch {epoch}: lr = {current_lr}")

if __name__ == "__main__":
    from jittor import nn
    from jittor.optim import SGD
    main()